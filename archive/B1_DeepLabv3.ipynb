{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.segmentation import deeplabv3\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from utils import data\n",
    "import pandas as pd\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "from torchvision import transforms\n",
    "import config\n",
    "from tqdm import tqdm\n",
    "import kornia\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Should be cuda:0 in colab and cpu in local.\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = config.PATH_TO_DATA\n",
    "\n",
    "train_test_split = pd.read_csv(\"dataset/data_split.csv\")\n",
    "train_split = train_test_split[train_test_split[\"split\"] == \"Train\"][\"sampleid\"].values\n",
    "test_split = train_test_split[train_test_split[\"split\"] == \"Test\"][\"sampleid\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(\n",
    "            (256, 256)\n",
    "        ),  # Optional: Resize the input PIL Image to the given size.\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "preprocess_mask = transforms.Compose(\n",
    "    [\n",
    "        kornia.geometry.Resize(\n",
    "            (256, 256), interpolation=\"nearest\"\n",
    "        ),  # Optional: Resize the input PIL Image to the given size.\n",
    "    ]\n",
    ")\n",
    "dataset = data.PlanetDataset(\n",
    "    data_dir=DATA_PATH,\n",
    "    bands=[0, 1, 2],\n",
    "    transform=preprocess,\n",
    "    target_transform=preprocess_mask,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((dataset.id2index))\n",
    "\n",
    "training_set = Subset(\n",
    "    dataset=dataset, indices=[dataset.id2index[image_id] for image_id in train_split]\n",
    ")\n",
    "test_set = Subset(\n",
    "    dataset=dataset, indices=[dataset.id2index[image_id] for image_id in test_split]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"The training split has {len(training_set)} samples ({len(training_set)/len(dataset):.2%} of the full dataset).\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "training_set[0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_model(backbone_model=\"mbv3\", num_classes=2):\n",
    "    \"\"\"Source : https://learnopencv.com/deep-learning-based-document-segmentation-using-semantic-segmentation-deeplabv3-on-custom-dataset/\"\"\"\n",
    "    weights = \"DEFAULT\"  # Initialize model with pre-trained weights.\n",
    "    if backbone_model == \"mbv3\":\n",
    "        model = deeplabv3.deeplabv3_mobilenet_v3_large(weights=weights)\n",
    "    elif backbone_model == \"r50\":\n",
    "        model = deeplabv3.deeplabv3_resnet50(weights=weights)\n",
    "    elif backbone_model == \"r101\":\n",
    "        model = deeplabv3.deeplabv3_resnet101(weights=weights)\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"Wrong backbone model passed. Must be one of 'mbv3', 'r50' and 'r101' \"\n",
    "        )\n",
    "\n",
    "    model.classifier[4] = nn.LazyConv2d(num_classes, 1)\n",
    "    model.aux_classifier[4] = nn.LazyConv2d(num_classes, 1)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "model = prepare_model(num_classes=2)\n",
    "\n",
    "\n",
    "def weighted_focal_loss(p, y_true, alpha=0.7, gamma=1):\n",
    "    \"\"\"Weighted Focal Loss as described in https://www.mdpi.com/2072-4292/14/19/4694#B26-remotesensing-14-04694\"\"\"\n",
    "    p = p[\"out\"].softmax(dim=1)[\n",
    "        :, 0, :, :\n",
    "    ]  # transform model output to probabilities. Following the paper, we consider only the probability of the POSITIVE class\n",
    "    loss = torch.mean(\n",
    "        y_true * (-alpha * (torch.ones(size=p.size()) - p) ** gamma * torch.log(p))\n",
    "        - (torch.ones(size=p.size()) - y_true)\n",
    "        * (1 - alpha)\n",
    "        * (p) ** gamma\n",
    "        * torch.log(p)\n",
    "    )\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 1\n",
    "params = {\"batch_size\": 4, \"shuffle\": True}\n",
    "learning_rate = 0.001\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "dataloader = DataLoader(training_set, **params)\n",
    "criterion = weighted_focal_loss\n",
    "# Loop over epochs\n",
    "loss_list = [0]\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=len(dataloader) * max_epochs)\n",
    "for epoch in range(max_epochs):\n",
    "    # Training\n",
    "    loss = 0\n",
    "    progress_bar = tqdm(total=len(dataloader), desc=f\"Loss: {loss:.5f}\")\n",
    "\n",
    "    for local_batch, local_labels in dataloader:\n",
    "        # Transfer to GPU\n",
    "        local_batch, local_labels = local_batch.to(device), local_labels.to(device)\n",
    "        outputs = model(local_batch)\n",
    "        loss = criterion(outputs, local_labels)  # Calculate the loss\n",
    "        loss_list.append(loss.item())\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()  # Clear gradients\n",
    "        loss.backward()  # Compute gradients\n",
    "        optimizer.step()  # Update weights\n",
    "        scheduler.step()  # Update learning rate\n",
    "        progress_bar.set_description(f\"Loss: {loss.item():.5f}\")\n",
    "        progress_bar.update(1)  # Manually update the progress bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(loss_list)\n",
    "ax.set_xlabel(\"Iter\")\n",
    "ax.set_ylabel(\"Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "test_dataloader = DataLoader(test_set, **params)\n",
    "# tqdm.close()\n",
    "\n",
    "# Assuming you have a trained model, test data, and labels\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "y_true = []  # True labels\n",
    "y_pred = []  # Predicted labels\n",
    "\n",
    "with torch.no_grad():\n",
    "    for local_batch, local_labels in tqdm(test_dataloader):\n",
    "        local_batch, local_labels = local_batch.to(device), local_labels.to(device)\n",
    "        outputs = model(local_batch)[\"out\"]\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        y_true.extend(local_labels.cpu().numpy())  # Collect true labels\n",
    "        y_pred.extend(predicted.cpu().numpy())  # Collect predicted labels\n",
    "\n",
    "# Calculate precision, recall, and F1 score\n",
    "y_pred = np.array(y_pred).flatten()\n",
    "y_true = np.array(y_true).flatten()\n",
    "precision = precision_score(y_true, y_pred, average=\"weighted\")\n",
    "recall = recall_score(y_true, y_pred, average=\"weighted\")\n",
    "f1 = f1_score(y_true, y_pred, average=\"weighted\")\n",
    "\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1 Score: {f1:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DeeplabV3 plus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.deeplabv3plus import fetch_deeplabv3\n",
    "from utils.data import create_dataloaders\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.nn import Softmax\n",
    "\n",
    "import config\n",
    "\n",
    "DATA_PATH = config.PATH_TO_DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, test_loader = create_dataloaders(\n",
    "    DATA_PATH, 4, bands=[0, 1, 2]\n",
    ")\n",
    "model = fetch_deeplabv3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_focal_loss(p, y_true, alpha=0.7, gamma=1):\n",
    "    \"\"\"Weighted Focal Loss as described in https://www.mdpi.com/2072-4292/14/19/4694#B26-remotesensing-14-04694\"\"\"\n",
    "    p = p.softmax(dim=1)[\n",
    "        :, 0, :, :\n",
    "    ]  # transform model output to probabilities. Following the paper, we consider only the probability of the POSITIVE class\n",
    "    loss = torch.mean(\n",
    "        y_true * (-alpha * (torch.ones(size=p.size()) - p) ** gamma * torch.log(p))\n",
    "        - (torch.ones(size=p.size()) - y_true)\n",
    "        * (1 - alpha)\n",
    "        * (p) ** gamma\n",
    "        * torch.log(p)\n",
    "    )\n",
    "    return loss\n",
    "\n",
    "\n",
    "learning_rate = 0.001\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = weighted_focal_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6549695073341837\n",
      "0.6330583274026962\n",
      "0.4862583705357143\n",
      "0.4813614819460505\n",
      "0.49746890943877553\n",
      "0.5140407216019831\n",
      "0.5688675860969388\n",
      "0.5472800879586919\n",
      "0.6203015385841837\n",
      "0.6115542827839496\n",
      "0.5283551897321429\n",
      "0.525923866370852\n"
     ]
    }
   ],
   "source": [
    "# before training\n",
    "from sklearn.metrics import f1_score\n",
    "import torch\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate():\n",
    "    m = Softmax(dim=1)\n",
    "    for i, (batch_x, batch_y) in enumerate(test_loader):\n",
    "        out = model(batch_x)\n",
    "        preds = m(out).argmax(dim=1).flatten().numpy()\n",
    "        ground_truth = batch_y.flatten().numpy()\n",
    "        acc = (preds == ground_truth).sum() / len(ground_truth)\n",
    "        print(acc)\n",
    "\n",
    "        f1 = f1_score(preds, ground_truth, average=\"weighted\")\n",
    "        print(f1)\n",
    "        if i >= 5:\n",
    "            break\n",
    "\n",
    "\n",
    "validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.00000:   0%|          | 0/258 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.00590:   8%|▊         | 21/258 [01:18<15:23,  3.90s/it]"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "device = \"cpu\"\n",
    "max_epochs = 1\n",
    "learning_rate = 0.001\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = weighted_focal_loss\n",
    "dataloader = train_loader\n",
    "\n",
    "\n",
    "# Loop over epochs\n",
    "loss_list = [0]\n",
    "scheduler = CosineAnnealingLR(\n",
    "    optimizer, T_max=len(dataloader.torch_loader) * max_epochs\n",
    ")\n",
    "for epoch in range(max_epochs):\n",
    "    # Training\n",
    "    loss = 0\n",
    "    progress_bar = tqdm(total=len(dataloader.torch_loader), desc=f\"Loss: {loss:.5f}\")\n",
    "\n",
    "    for i, (local_batch, local_labels) in enumerate(dataloader):\n",
    "        # Transfer to GPU\n",
    "        local_batch, local_labels = local_batch.to(device), local_labels.to(device)\n",
    "        outputs = model(local_batch)\n",
    "        loss = criterion(outputs, local_labels)  # Calculate the loss\n",
    "        loss_list.append(loss.item())\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()  # Clear gradients\n",
    "        loss.backward()  # Compute gradients\n",
    "        optimizer.step()  # Update weights\n",
    "        scheduler.step()  # Update learning rate\n",
    "        progress_bar.set_description(f\"Loss: {loss.item():.5f}\")\n",
    "        progress_bar.update(1)  # Manually update the progress bar\n",
    "\n",
    "        if i >= 20:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7200952646683674\n",
      "0.7841181017732856\n",
      "0.2683902662627551\n",
      "0.4083750028728553\n",
      "0.5371741470025511\n",
      "0.6722778449771893\n",
      "0.7111218510841837\n",
      "0.7858992968100925\n",
      "0.6607840401785714\n",
      "0.7515817295525145\n",
      "0.2718082350127551\n",
      "0.3969848346868168\n"
     ]
    }
   ],
   "source": [
    "validate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ipeo_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
