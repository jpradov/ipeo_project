{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from utils.visualisation import visualise_image, plot_band_distribution\n",
    "from utils.data import create_dataloaders, PlanetDataset\n",
    "import torch\n",
    "import config\n",
    "\n",
    "device = config.device\n",
    "DATA_PATH = config.PATH_TO_DATA\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Illustrate Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, test_loader = create_dataloaders(DATA_PATH, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def illustrate_data_loader(loader, show_n_images: int):\n",
    "    for i, (batch_sample, batch_masks) in enumerate(loader):\n",
    "        image, label = batch_sample[0], batch_masks[0]\n",
    "        image, label = torch.permute(image, (1, 2, 0)), torch.permute(label, (1, 2, 0))\n",
    "        visualise_image(image.numpy(), label.numpy())\n",
    "        if i >= show_n_images:\n",
    "            return\n",
    "        \n",
    "def speedtest_dataloader(size, same, num_workers=0, ):\n",
    "    train_loader, _, _ = create_dataloaders(DATA_PATH, batch_size=size, batch_transforms=same, num_workers=num_workers)\n",
    "    for x, y in train_loader: # iterate through one batch\n",
    "        pass\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speedtest_dataloader(4, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit speedtest_dataloader(4, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit speedtest_dataloader(32, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit speedtest_dataloader(32, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit speedtest_dataloader(64, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit speedtest_dataloader(64, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit speedtest_dataloader(64, True, 4) # with concurrency - slower; overhead is taking over if we are just loading the image without a pass through the net?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "illustrate_data_loader(train_loader, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "illustrate_data_loader(val_loader, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "illustrate_data_loader(test_loader, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Illustrate distribution of raw values across bands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = PlanetDataset(data_dir=DATA_PATH, bands=[0,1,2,3])\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=False, sampler=None,\n",
    "           batch_sampler=None, num_workers=0)\n",
    "dataloader = iter(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample, mask = next(dataloader)\n",
    "for batch_num in range(4):\n",
    "    plot_band_distribution(sample[batch_num,:,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-calculated means, std, mins, max of raw images calculated on the full test set:\n",
    "means = torch.tensor([ 265.7371,  445.2234,  393.7881, 2773.2734])\n",
    "stds = torch.tensor([ 91.8786, 110.0122, 191.7516, 709.2327])\n",
    "mins = torch.tensor([ 0., 21.,  6., 77.])\n",
    "max = torch.tensor([ 4433.,  5023.,  8230., 10000.])\n",
    "\n",
    "\"\"\"\n",
    "means = torch.mean(train_sample.float(), dim=(0, 1, 2))\n",
    "std = torch.std(train_sample.float(), dim=(0, 1, 2))\n",
    "min = torch.amin(train_sample.float(), dim=(0, 1, 2))\n",
    "max = torch.amax(train_sample.float(), dim=(0, 1, 2))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLE Predictions based on Mutlivariate Gauassian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, test_loader = create_dataloaders(DATA_PATH, batch_size=2000, transforms=True) # get full train and val dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(X_in, y_in):\n",
    "    X_out = torch.permute(X_in, (1, 0, 2, 3)).flatten(start_dim=1).T\n",
    "    y_out = y_in.flatten(start_dim=0)\n",
    "\n",
    "    assert X_out.shape[0] == len(y_out)\n",
    "    print(\"Dimensions of transformed output: \")\n",
    "    print(\"Data: {}\".format(X_out.shape))\n",
    "    print(\"Labels: {}\".format(y_out.shape))\n",
    "    return X_out, y_out\n",
    "\n",
    "def pull_train_loader(loader, n_pulls):\n",
    "    image_batches = []\n",
    "    mask_batches = []\n",
    "    for _ in range(n_pulls):\n",
    "        batch_x, batch_y = next(iter(loader))\n",
    "        image_batches.append(batch_x)\n",
    "        mask_batches.append(batch_y)\n",
    "\n",
    "    X = torch.cat(image_batches, 0)\n",
    "    y = torch.cat(mask_batches, 0)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = pull_train_loader(train_loader, 1)\n",
    "X_test, y_test = next(iter(test_loader))\n",
    "X_val, y_val = next(iter(val_loader))\n",
    "\n",
    "X_train, y_train = prepare_data(X_train, y_train)\n",
    "X_test, y_test = prepare_data(X_test, y_test)\n",
    "X_val, y_val = prepare_data(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class MLE_model():\n",
    "    def __init__(self, ):\n",
    "        self.pos_means = None\n",
    "        self.neg_means = None\n",
    "        self.pos_cov = None\n",
    "        self.neg_cov = None\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        \n",
    "        # seperate data into both classes\n",
    "        pos = (y_train==1)\n",
    "        neg = (y_train==0)\n",
    "\n",
    "        X_train_pos = X_train[pos, :]\n",
    "        X_train_neg = X_train[neg, :]\n",
    "\n",
    "        # fit multivariate gaussian distributions for both classes\n",
    "        self.pos_means = torch.mean(X_train_pos, dim=0).numpy()\n",
    "        self.pos_cov = np.cov(X_train_pos.numpy().T)\n",
    "\n",
    "        self.neg_means = torch.mean(X_train_neg, dim=0).numpy()\n",
    "        self.neg_cov = np.cov(X_train_neg.numpy().T)\n",
    "\n",
    "        # validate fitting process\n",
    "        print(\"Model Fitted with means {} and {}\".format(self.pos_means, self.neg_means))\n",
    "        print(\"Covariance matrix are \\n {} \\n and \\n {}\".format(self.pos_cov, self.neg_cov))\n",
    "        return\n",
    "\n",
    "    def predict(self, X_test):\n",
    "\n",
    "        # convert to numpy\n",
    "        X_test = X_test.numpy()\n",
    "\n",
    "        # Run MLE estimator\n",
    "        # calculate log odds for both classes\n",
    "        log_ps = [self.compute_log_p_solution(X_test, m, s) for m, s in zip([self.neg_means, self.pos_means], [self.neg_cov, self.pos_cov])]\n",
    "        \n",
    "        # take argmax\n",
    "        assignments = np.argmax(log_ps, axis=0)\n",
    "        return log_ps, assignments\n",
    "    \n",
    "    def compute_log_p_solution(self, X, mean, sigma):\n",
    "        d = X.shape[1]\n",
    "        c = -np.log(2 * np.pi) * (d / 2) - 0.5 * np.log(np.linalg.det(sigma))\n",
    "        A = X - mean\n",
    "        invSigma = np.linalg.inv(sigma)\n",
    "\n",
    "        return -0.5 * np.sum(A * (A.dot(invSigma)), axis=1) + c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and predict\n",
    "model = MLE_model()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "log_ps, pred_tot = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate on test set\n",
    "from sklearn.metrics import accuracy_score, jaccard_score, precision_score, recall_score, f1_score\n",
    "\n",
    "accuracy = accuracy_score(y_test, pred_tot)   \n",
    "jaccard = jaccard_score(y_test, pred_tot, average='macro') # equivalent to IOU\n",
    "precision = precision_score(y_test, pred_tot, average='macro')\n",
    "recall = recall_score(y_test, pred_tot, average='macro')\n",
    "f1 = f1_score(y_test, pred_tot, average='macro') # equivalent to Dice index\n",
    "\n",
    "print(\"Accuracy: {:.4}\".format(accuracy))\n",
    "print(\"F1: {:.4}\".format(f1))\n",
    "print(\"Jaccard: {:.4}\".format(jaccard))\n",
    "print(\"Precision: {:.4}\".format(precision))\n",
    "print(\"Recall: {:.4}\".format(recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate on val set\n",
    "log_ps, pred_tot = model.predict(X_val)\n",
    "\n",
    "accuracy = accuracy_score(y_val, pred_tot)   \n",
    "jaccard = jaccard_score(y_val, pred_tot, average='macro') # equivalent to IOU\n",
    "precision = precision_score(y_val, pred_tot, average='macro')\n",
    "recall = recall_score(y_val, pred_tot, average='macro')\n",
    "f1 = f1_score(y_val, pred_tot, average='macro') # equivalent to Dice index\n",
    "\n",
    "print(\"Accuracy: {:.4}\".format(accuracy))\n",
    "print(\"F1: {:.4}\".format(f1))\n",
    "print(\"Jaccard: {:.4}\".format(jaccard))\n",
    "print(\"Precision: {:.4}\".format(precision))\n",
    "print(\"Recall: {:.4}\".format(recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training performance\n",
    "log_ps, pred_tot = model.predict(X_train)\n",
    "\n",
    "accuracy = accuracy_score(y_train, pred_tot)   \n",
    "jaccard = jaccard_score(y_train, pred_tot, average='macro') # equivalent to IOU\n",
    "precision = precision_score(y_train, pred_tot, average='macro')\n",
    "recall = recall_score(y_train, pred_tot, average='macro')\n",
    "f1 = f1_score(y_train, pred_tot, average='macro') # equivalent to Dice index\n",
    "\n",
    "print(\"Accuracy: {:.4}\".format(accuracy))\n",
    "print(\"F1: {:.4}\".format(f1))\n",
    "print(\"Jaccard: {:.4}\".format(jaccard))\n",
    "print(\"Precision: {:.4}\".format(precision))\n",
    "print(\"Recall: {:.4}\".format(recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing unsupervised methods for good measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "model = GaussianMixture(n_components=2, verbose=True)\n",
    "\n",
    "n_samples = 20_000_000\n",
    "indices = np.random.choice(np.arange(n_samples), size=n_samples)\n",
    "\n",
    "X_train_ = X_train[indices, :]\n",
    "\n",
    "model.fit(X_train_)\n",
    "\n",
    "print(\"Found means: \", model.means_)\n",
    "print(\"Found covariances: \", model.covariances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_tot = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(print(pred_tot.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since this is unspervised, we need to flip the class (if accuracy on your machine is bad, just flip it back by rerunning this cell)\n",
    "pred_tot = np.where(pred_tot==1, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y_test, pred_tot)\n",
    "jaccard = jaccard_score(y_test, pred_tot, average='macro') # equivalent to IOU\n",
    "precision = precision_score(y_test, pred_tot, average='macro')\n",
    "recall = recall_score(y_test, pred_tot, average='macro')\n",
    "f1 = f1_score(y_test, pred_tot, average='macro') # equivalent to Dice index\n",
    "\n",
    "print(\"Accuracy: {:.4}\".format(accuracy))\n",
    "print(\"F1: {:.4}\".format(f1))\n",
    "print(\"Jaccard: {:.4}\".format(jaccard))\n",
    "print(\"Precision: {:.4}\".format(precision))\n",
    "print(\"Recall: {:.4}\".format(recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --> we outperform any existing paper and our own NN without even using labels\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "segment_2_deforest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
