{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.segmentation import deeplabv3\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from utils import data\n",
    "import pandas as pd\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "from torchvision import transforms\n",
    "import config\n",
    "from tqdm import tqdm\n",
    "import kornia\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from models.msnet import MSNet\n",
    "from torch.utils.data import DataLoader\n",
    "from utils.data import create_dataloaders\n",
    "from utils.train import train_epoch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "#Should be cuda:0 in colab and cpu in local.\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "DATA_PATH = config.PATH_TO_DATA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_params = {\n",
    "    \"batch_size\" : 4,\n",
    "    \"batch_transforms\" : True,\n",
    "    \"num_workers\" : 1\n",
    "}\n",
    "train_loader, test_loader, val_loader = create_dataloaders(DATA_PATH, **loader_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.72286:   0%|          | 1/258 [00:02<09:06,  2.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1-0 batch_loss=1.81e-01 batch_acc=24834.250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.34181:  10%|█         | 26/258 [01:27<14:16,  3.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1-25 batch_loss=8.55e-02 batch_acc=41041.250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.43099:  20%|█▉        | 51/258 [02:56<14:02,  4.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1-50 batch_loss=1.08e-01 batch_acc=41289.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.38162:  29%|██▉       | 76/258 [04:37<11:29,  3.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1-75 batch_loss=9.54e-02 batch_acc=39556.750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.42797:  39%|███▉      | 101/258 [06:11<09:36,  3.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1-100 batch_loss=1.07e-01 batch_acc=40348.250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.24076:  49%|████▉     | 126/258 [07:46<08:26,  3.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1-125 batch_loss=6.02e-02 batch_acc=46216.250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.33882:  59%|█████▊    | 151/258 [09:19<06:34,  3.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1-150 batch_loss=8.47e-02 batch_acc=43149.750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.28682:  68%|██████▊   | 176/258 [11:08<05:58,  4.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1-175 batch_loss=7.17e-02 batch_acc=42453.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.33012:  78%|███████▊  | 201/258 [12:43<03:32,  3.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1-200 batch_loss=8.25e-02 batch_acc=43784.250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.25952:  88%|████████▊ | 226/258 [14:23<02:12,  4.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1-225 batch_loss=6.49e-02 batch_acc=44204.500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.35437:  97%|█████████▋| 251/258 [15:59<00:26,  3.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1-250 batch_loss=8.86e-02 batch_acc=40743.750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.38816: 100%|██████████| 258/258 [16:25<00:00,  3.82s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0.7228601574897766,\n",
       "  0.5597561001777649,\n",
       "  0.5106697678565979,\n",
       "  0.6446294784545898,\n",
       "  0.4854682683944702,\n",
       "  0.5636721253395081,\n",
       "  0.5343805551528931,\n",
       "  0.40266701579093933,\n",
       "  0.34239184856414795,\n",
       "  0.5307309627532959,\n",
       "  0.5543964505195618,\n",
       "  0.4451117217540741,\n",
       "  0.43413084745407104,\n",
       "  0.34156954288482666,\n",
       "  0.40383467078208923,\n",
       "  0.3446826636791229,\n",
       "  0.42863667011260986,\n",
       "  0.33199045062065125,\n",
       "  0.3499559760093689,\n",
       "  0.37595275044441223,\n",
       "  0.3960740566253662,\n",
       "  0.4260420501232147,\n",
       "  0.3897898197174072,\n",
       "  0.622051477432251,\n",
       "  0.3968379497528076,\n",
       "  0.3418055474758148,\n",
       "  0.48386141657829285,\n",
       "  0.3364883363246918,\n",
       "  0.290660560131073,\n",
       "  0.3002582788467407,\n",
       "  0.32818666100502014,\n",
       "  0.28354933857917786,\n",
       "  0.26751708984375,\n",
       "  0.5237176418304443,\n",
       "  0.32343366742134094,\n",
       "  0.2891370356082916,\n",
       "  0.38988402485847473,\n",
       "  0.3976903557777405,\n",
       "  0.5046840310096741,\n",
       "  0.2206658124923706,\n",
       "  0.31937623023986816,\n",
       "  0.3425326347351074,\n",
       "  0.2724638879299164,\n",
       "  0.36810964345932007,\n",
       "  0.25012531876564026,\n",
       "  0.2497732788324356,\n",
       "  0.455706387758255,\n",
       "  0.4046381711959839,\n",
       "  0.36792436242103577,\n",
       "  0.2596796452999115,\n",
       "  0.4309910237789154,\n",
       "  0.41866162419319153,\n",
       "  0.4520334005355835,\n",
       "  0.20696066319942474,\n",
       "  0.7722529768943787,\n",
       "  0.271499365568161,\n",
       "  0.2938402593135834,\n",
       "  0.2989807426929474,\n",
       "  0.48742589354515076,\n",
       "  0.2647485136985779,\n",
       "  0.7252116799354553,\n",
       "  0.3059356212615967,\n",
       "  0.6250296235084534,\n",
       "  1.475621223449707,\n",
       "  0.49988216161727905,\n",
       "  0.4218977987766266,\n",
       "  0.3384743630886078,\n",
       "  0.3868359625339508,\n",
       "  0.3610959053039551,\n",
       "  0.39976656436920166,\n",
       "  0.4002148509025574,\n",
       "  0.38860955834388733,\n",
       "  0.3506527841091156,\n",
       "  0.4376088082790375,\n",
       "  0.36166560649871826,\n",
       "  0.3816249370574951,\n",
       "  0.47191059589385986,\n",
       "  0.29704898595809937,\n",
       "  0.3632078468799591,\n",
       "  0.394396036863327,\n",
       "  0.46368318796157837,\n",
       "  0.3165889382362366,\n",
       "  0.3612684905529022,\n",
       "  0.5241174697875977,\n",
       "  0.38646847009658813,\n",
       "  1.432979941368103,\n",
       "  0.31829506158828735,\n",
       "  0.40136995911598206,\n",
       "  0.4002733528614044,\n",
       "  0.29421383142471313,\n",
       "  0.3172740042209625,\n",
       "  0.43584635853767395,\n",
       "  0.3144618570804596,\n",
       "  0.3025464713573456,\n",
       "  0.4373542368412018,\n",
       "  0.3961241841316223,\n",
       "  0.327627569437027,\n",
       "  0.3764808475971222,\n",
       "  0.3536716401576996,\n",
       "  0.38506177067756653,\n",
       "  0.4279720187187195,\n",
       "  0.28774258494377136,\n",
       "  0.2884507477283478,\n",
       "  0.36717987060546875,\n",
       "  0.2558495104312897,\n",
       "  0.28445544838905334,\n",
       "  0.31501078605651855,\n",
       "  0.7555640339851379,\n",
       "  0.22425396740436554,\n",
       "  0.343935489654541,\n",
       "  0.256048321723938,\n",
       "  0.35233521461486816,\n",
       "  0.227487713098526,\n",
       "  0.3961799740791321,\n",
       "  0.6514277458190918,\n",
       "  0.4050469994544983,\n",
       "  0.3068820536136627,\n",
       "  0.2306564450263977,\n",
       "  0.2715970575809479,\n",
       "  0.21372602880001068,\n",
       "  0.7884925007820129,\n",
       "  0.5302358865737915,\n",
       "  0.2860516607761383,\n",
       "  0.23291321098804474,\n",
       "  0.5887110829353333,\n",
       "  0.24075831472873688,\n",
       "  0.543784499168396,\n",
       "  0.3456035852432251,\n",
       "  0.32303106784820557,\n",
       "  0.5762001872062683,\n",
       "  0.3189046084880829,\n",
       "  0.47156330943107605,\n",
       "  0.37407177686691284,\n",
       "  0.46568071842193604,\n",
       "  0.2963411509990692,\n",
       "  0.3091743290424347,\n",
       "  0.39004194736480713,\n",
       "  0.7438816428184509,\n",
       "  0.40468376874923706,\n",
       "  0.2458120584487915,\n",
       "  0.36159956455230713,\n",
       "  0.2695600390434265,\n",
       "  0.23172448575496674,\n",
       "  0.32036712765693665,\n",
       "  0.30332961678504944,\n",
       "  0.3029579520225525,\n",
       "  0.29518014192581177,\n",
       "  0.33136531710624695,\n",
       "  0.32584378123283386,\n",
       "  0.3256773054599762,\n",
       "  0.33882054686546326,\n",
       "  0.4906335175037384,\n",
       "  0.4159167408943176,\n",
       "  0.4598158895969391,\n",
       "  0.4486415386199951,\n",
       "  0.2976866364479065,\n",
       "  0.25880274176597595,\n",
       "  0.25619077682495117,\n",
       "  0.2732590138912201,\n",
       "  0.4100286066532135,\n",
       "  0.20420341193675995,\n",
       "  0.27663949131965637,\n",
       "  0.29731830954551697,\n",
       "  0.34112945199012756,\n",
       "  0.23401688039302826,\n",
       "  0.36665982007980347,\n",
       "  0.31733572483062744,\n",
       "  0.515965461730957,\n",
       "  0.5032477378845215,\n",
       "  0.37544772028923035,\n",
       "  0.29239293932914734,\n",
       "  0.5025116205215454,\n",
       "  0.40520885586738586,\n",
       "  0.6415116190910339,\n",
       "  0.2725813388824463,\n",
       "  0.28682461380958557,\n",
       "  0.9771003127098083,\n",
       "  0.321306437253952,\n",
       "  0.34307071566581726,\n",
       "  0.26034945249557495,\n",
       "  0.31257545948028564,\n",
       "  0.2119617611169815,\n",
       "  0.4219440817832947,\n",
       "  0.39083775877952576,\n",
       "  0.31480711698532104,\n",
       "  0.28988561034202576,\n",
       "  0.3283161222934723,\n",
       "  0.3061929941177368,\n",
       "  0.2341729998588562,\n",
       "  0.24918460845947266,\n",
       "  0.8310413956642151,\n",
       "  0.4480925500392914,\n",
       "  0.2723521292209625,\n",
       "  0.45125746726989746,\n",
       "  0.35861027240753174,\n",
       "  0.5457715392112732,\n",
       "  0.3149850368499756,\n",
       "  0.5568923354148865,\n",
       "  0.6440088748931885,\n",
       "  0.27696576714515686,\n",
       "  0.33011502027511597,\n",
       "  0.3428676724433899,\n",
       "  0.3564402461051941,\n",
       "  0.27617666125297546,\n",
       "  0.2696071267127991,\n",
       "  0.3133488595485687,\n",
       "  0.30455514788627625,\n",
       "  0.25564146041870117,\n",
       "  0.33957305550575256,\n",
       "  0.42711904644966125,\n",
       "  0.24870078265666962,\n",
       "  0.2757573127746582,\n",
       "  0.2675548791885376,\n",
       "  0.27440011501312256,\n",
       "  0.19819213449954987,\n",
       "  0.3681453466415405,\n",
       "  0.3334112763404846,\n",
       "  0.3713871240615845,\n",
       "  0.292207270860672,\n",
       "  0.48231545090675354,\n",
       "  0.5311868786811829,\n",
       "  0.3118247389793396,\n",
       "  0.2150612324476242,\n",
       "  0.2521715760231018,\n",
       "  0.31753823161125183,\n",
       "  0.25952279567718506,\n",
       "  0.3974570035934448,\n",
       "  0.525821328163147,\n",
       "  0.3075423836708069,\n",
       "  0.5487444996833801,\n",
       "  0.288937509059906,\n",
       "  0.24842916429042816,\n",
       "  0.19880902767181396,\n",
       "  0.23446154594421387,\n",
       "  0.3510388433933258,\n",
       "  0.3062390387058258,\n",
       "  0.3623446524143219,\n",
       "  0.5290108323097229,\n",
       "  0.34014514088630676,\n",
       "  0.3500058352947235,\n",
       "  0.4204132556915283,\n",
       "  0.39563512802124023,\n",
       "  0.392296701669693,\n",
       "  0.3257046043872833,\n",
       "  0.46075284481048584,\n",
       "  0.3920787572860718,\n",
       "  0.3263506591320038,\n",
       "  0.2903718650341034,\n",
       "  0.2953990399837494,\n",
       "  0.21986807882785797,\n",
       "  0.35437193512916565,\n",
       "  0.32239776849746704,\n",
       "  0.2372666597366333,\n",
       "  0.3130953013896942,\n",
       "  0.3447343111038208,\n",
       "  0.34246739745140076,\n",
       "  0.25733381509780884,\n",
       "  0.3881607949733734],\n",
       " [24834.25,\n",
       "  35828.5,\n",
       "  36762.0,\n",
       "  32753.25,\n",
       "  40395.75,\n",
       "  35338.5,\n",
       "  35994.25,\n",
       "  40707.5,\n",
       "  46154.5,\n",
       "  36984.25,\n",
       "  35000.5,\n",
       "  38229.75,\n",
       "  40862.25,\n",
       "  41329.0,\n",
       "  38777.25,\n",
       "  41569.75,\n",
       "  39384.25,\n",
       "  43706.0,\n",
       "  41322.75,\n",
       "  41903.0,\n",
       "  38708.25,\n",
       "  37087.5,\n",
       "  38714.75,\n",
       "  35072.5,\n",
       "  42049.0,\n",
       "  41041.25,\n",
       "  37769.75,\n",
       "  42227.5,\n",
       "  43067.75,\n",
       "  44268.75,\n",
       "  43988.75,\n",
       "  44488.75,\n",
       "  43673.5,\n",
       "  35680.0,\n",
       "  42969.0,\n",
       "  44613.0,\n",
       "  41535.0,\n",
       "  39902.75,\n",
       "  39194.0,\n",
       "  46228.0,\n",
       "  43105.25,\n",
       "  41684.75,\n",
       "  44414.25,\n",
       "  40876.25,\n",
       "  45409.25,\n",
       "  45763.5,\n",
       "  38708.5,\n",
       "  40510.0,\n",
       "  43349.0,\n",
       "  43975.25,\n",
       "  41289.0,\n",
       "  41081.5,\n",
       "  39256.5,\n",
       "  46177.0,\n",
       "  29027.75,\n",
       "  45196.5,\n",
       "  43634.5,\n",
       "  43761.75,\n",
       "  35976.25,\n",
       "  43486.5,\n",
       "  32478.75,\n",
       "  44787.25,\n",
       "  33981.25,\n",
       "  26483.0,\n",
       "  40395.5,\n",
       "  39069.0,\n",
       "  40937.75,\n",
       "  40893.5,\n",
       "  43532.75,\n",
       "  40198.25,\n",
       "  38506.5,\n",
       "  44341.0,\n",
       "  42931.5,\n",
       "  36305.5,\n",
       "  41714.0,\n",
       "  39556.75,\n",
       "  34624.0,\n",
       "  43917.75,\n",
       "  42190.25,\n",
       "  39906.25,\n",
       "  41752.25,\n",
       "  42491.75,\n",
       "  44096.25,\n",
       "  35883.5,\n",
       "  40074.25,\n",
       "  28782.25,\n",
       "  43747.75,\n",
       "  41352.75,\n",
       "  40575.75,\n",
       "  43917.5,\n",
       "  42654.0,\n",
       "  39845.25,\n",
       "  44807.0,\n",
       "  44316.75,\n",
       "  39889.75,\n",
       "  41483.5,\n",
       "  42403.75,\n",
       "  41351.75,\n",
       "  43700.5,\n",
       "  41923.0,\n",
       "  40348.25,\n",
       "  45075.25,\n",
       "  44417.5,\n",
       "  42834.0,\n",
       "  44817.75,\n",
       "  44406.75,\n",
       "  43987.0,\n",
       "  34054.25,\n",
       "  46293.0,\n",
       "  42824.5,\n",
       "  45567.0,\n",
       "  40684.25,\n",
       "  46011.25,\n",
       "  41035.25,\n",
       "  34365.75,\n",
       "  40500.5,\n",
       "  42965.5,\n",
       "  45417.75,\n",
       "  44207.5,\n",
       "  46203.0,\n",
       "  34863.75,\n",
       "  34695.0,\n",
       "  43539.5,\n",
       "  45657.75,\n",
       "  34492.25,\n",
       "  46216.25,\n",
       "  32225.25,\n",
       "  42188.75,\n",
       "  43179.25,\n",
       "  34947.75,\n",
       "  42408.75,\n",
       "  37945.5,\n",
       "  42053.25,\n",
       "  39171.25,\n",
       "  45172.0,\n",
       "  43364.75,\n",
       "  43573.75,\n",
       "  30623.75,\n",
       "  41416.0,\n",
       "  46231.25,\n",
       "  41063.0,\n",
       "  46423.0,\n",
       "  46371.25,\n",
       "  44512.25,\n",
       "  42933.0,\n",
       "  42833.75,\n",
       "  43791.5,\n",
       "  43373.75,\n",
       "  42931.5,\n",
       "  41550.25,\n",
       "  43149.75,\n",
       "  36711.75,\n",
       "  44296.0,\n",
       "  39344.25,\n",
       "  36901.0,\n",
       "  46656.75,\n",
       "  44527.5,\n",
       "  45037.25,\n",
       "  45436.5,\n",
       "  40502.75,\n",
       "  47006.5,\n",
       "  44210.75,\n",
       "  43080.25,\n",
       "  41519.75,\n",
       "  46421.5,\n",
       "  41488.25,\n",
       "  43428.5,\n",
       "  35225.5,\n",
       "  39976.0,\n",
       "  45338.75,\n",
       "  44813.25,\n",
       "  36403.5,\n",
       "  40754.75,\n",
       "  33528.75,\n",
       "  44015.0,\n",
       "  42453.0,\n",
       "  30707.5,\n",
       "  42920.5,\n",
       "  42192.25,\n",
       "  44785.75,\n",
       "  42444.25,\n",
       "  47206.0,\n",
       "  37157.75,\n",
       "  40515.0,\n",
       "  42320.75,\n",
       "  42312.75,\n",
       "  40633.75,\n",
       "  42702.0,\n",
       "  46179.75,\n",
       "  44899.25,\n",
       "  30497.0,\n",
       "  39114.0,\n",
       "  43871.75,\n",
       "  36748.5,\n",
       "  41675.75,\n",
       "  36206.0,\n",
       "  44123.75,\n",
       "  34728.0,\n",
       "  37870.0,\n",
       "  45205.0,\n",
       "  43784.25,\n",
       "  42398.0,\n",
       "  43517.25,\n",
       "  43971.5,\n",
       "  44894.0,\n",
       "  45072.0,\n",
       "  43354.5,\n",
       "  45817.0,\n",
       "  42689.0,\n",
       "  39328.5,\n",
       "  45787.25,\n",
       "  45202.5,\n",
       "  45312.75,\n",
       "  44297.0,\n",
       "  46916.75,\n",
       "  41756.25,\n",
       "  42855.25,\n",
       "  41263.75,\n",
       "  44318.25,\n",
       "  37960.75,\n",
       "  37629.5,\n",
       "  43744.75,\n",
       "  46806.25,\n",
       "  45897.5,\n",
       "  43793.25,\n",
       "  44204.5,\n",
       "  41022.5,\n",
       "  36701.75,\n",
       "  44311.25,\n",
       "  35684.75,\n",
       "  43134.5,\n",
       "  44912.0,\n",
       "  46515.75,\n",
       "  45433.75,\n",
       "  41593.5,\n",
       "  43396.5,\n",
       "  42359.5,\n",
       "  35430.25,\n",
       "  42499.5,\n",
       "  40789.5,\n",
       "  39552.5,\n",
       "  38282.75,\n",
       "  43510.5,\n",
       "  41014.75,\n",
       "  40217.75,\n",
       "  40834.0,\n",
       "  40765.75,\n",
       "  43175.75,\n",
       "  43677.25,\n",
       "  45280.25,\n",
       "  40743.75,\n",
       "  42450.25,\n",
       "  45891.0,\n",
       "  43903.25,\n",
       "  41488.75,\n",
       "  45374.0,\n",
       "  44809.25,\n",
       "  40641.0])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "max_epochs = 1\n",
    "params = {'batch_size': 4,\n",
    "          'shuffle': True}\n",
    "model = MSNet(num_classes=2)\n",
    "learning_rate = 0.001 \n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "loss_history, accuracy_history = train_epoch(model=model,device=device,train_loader=train_loader,optimizer=optimizer,epoch=1,criterion=criterion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "75it [01:31,  1.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.89\n",
      "Recall: 0.88\n",
      "F1 Score: 0.88\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "#tqdm.close()\n",
    "\n",
    "# Assuming you have a trained model, test data, and labels\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "y_true = []  # True labels\n",
    "y_pred = []  # Predicted labels\n",
    "\n",
    "with torch.no_grad():\n",
    "    for local_batch, local_labels in tqdm(test_loader, total=len(test_loader.torch_loader)):\n",
    "        local_batch, local_labels = local_batch.to(device), local_labels.to(device)\n",
    "        outputs = model(local_batch)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        \n",
    "        y_true.extend(local_labels.cpu().numpy())  # Collect true labels\n",
    "        y_pred.extend(predicted.cpu().numpy())  # Collect predicted labels\n",
    "\n",
    "# Calculate precision, recall, and F1 score\n",
    "y_pred = np.array(y_pred).flatten()\n",
    "y_true = np.array(y_true).flatten()\n",
    "precision = precision_score(y_true, y_pred, average='weighted')\n",
    "recall = recall_score(y_true, y_pred, average='weighted')\n",
    "f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "\n",
    "print(f'Precision: {precision:.2f}')\n",
    "print(f'Recall: {recall:.2f}')\n",
    "print(f'F1 Score: {f1:.2f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ipeo_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
